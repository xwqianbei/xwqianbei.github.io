<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"xwqianbei.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING ICLR-24 Nanjing UniversityAbstract Challenge: 智能体之间的有效合作 motivation: 受角色和智能体行为模式之间相关性的启发 ACORM: Attention-guid">
<meta property="og:type" content="article">
<meta property="og:title" content="ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING">
<meta property="og:url" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/index.html">
<meta property="og:site_name" content="米兰的小铁酱">
<meta property="og:description" content="ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING ICLR-24 Nanjing UniversityAbstract Challenge: 智能体之间的有效合作 motivation: 受角色和智能体行为模式之间相关性的启发 ACORM: Attention-guid">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886563461.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886677274.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886690507.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886699683.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886709036.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709887238174.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709893541789.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709895486679.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709897074004.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709908700732.png">
<meta property="og:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709909066167.png">
<meta property="article:published_time" content="2024-03-04T13:27:25.000Z">
<meta property="article:modified_time" content="2024-03-08T15:31:56.778Z">
<meta property="article:author" content="Wei">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886563461.png">


<link rel="canonical" href="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/","path":"2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/","title":"ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING | 米兰的小铁酱</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">米兰的小铁酱</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING-ICLR-24-Nanjing-University"><span class="nav-number">1.</span> <span class="nav-text">ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING ICLR-24 Nanjing University</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Method"><span class="nav-number">1.3.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Contrastive-Role-Representations"><span class="nav-number">1.3.1.</span> <span class="nav-text">Contrastive Role Representations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ATTENTION-GUIDED-ROLE-COORDINATION"><span class="nav-number">1.3.2.</span> <span class="nav-text">ATTENTION-GUIDED ROLE COORDINATION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Algorithm"><span class="nav-number">1.3.3.</span> <span class="nav-text">Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiments"><span class="nav-number">1.4.</span> <span class="nav-text">Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-efficiency-and-stability"><span class="nav-number">1.4.1.</span> <span class="nav-text">3.1 efficiency and stability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-CONTRASTIVE-ROLE-REPRESENTATIONS"><span class="nav-number">1.4.2.</span> <span class="nav-text">3.2 CONTRASTIVE ROLE REPRESENTATIONS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-ATTENTION-GUIDED-ROLE-COORDINATION"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.3 ATTENTION-GUIDED ROLE COORDINATION</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wei</p>
  <div class="site-description" itemprop="description">attention is all you need</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xwqianbei.github.io/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wei">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="米兰的小铁酱">
      <meta itemprop="description" content="attention is all you need">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING | 米兰的小铁酱">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-04 21:27:25" itemprop="dateCreated datePublished" datetime="2024-03-04T21:27:25+08:00">2024-03-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-08 23:31:56" itemprop="dateModified" datetime="2024-03-08T23:31:56+08:00">2024-03-08</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING-ICLR-24-Nanjing-University"><a href="#ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING-ICLR-24-Nanjing-University" class="headerlink" title="ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING ICLR-24 Nanjing University"></a>ATTENTION-GUIDED CONTRASTIVE ROLE REPRE-SENTATIONS FOR MULTI-AGENT REINFORCEMENT LEARNING ICLR-24 Nanjing University</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><strong>Challenge</strong>: 智能体之间的有效合作</li>
<li><strong>motivation</strong>: 受角色和智能体行为模式之间相关性的启发</li>
<li><strong>ACORM</strong>: Attention-guided COntrastive Role representation learning for MARL (ACORM) -&gt; 促进智能体之间 行为异质化、知识传输、技能上的协调</li>
<li><strong>methods</strong>: <ol>
<li>使用最大化互信息来形式化角色表征学习(role representation learning) -&gt; 推导出对比学习目标 -&gt; 精简估计负样本的分布 </li>
<li>利用注意力机制促进在价值分解中global state 关注到 学习到的role representations -&gt; 隐式地指导智能体协调在一个技能性的skillful role space 以生成具有表现力的信用分配</li>
</ol>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><strong>challenge</strong>: 共享policy参数 加速massive agents场景下的 合作学习 -&gt; 导致同质的行为 阻碍了多样性探索和复杂的合作 </li>
<li><strong>existing works</strong>: Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition -&gt; 利用对比学习使得agent对应的identify representation 相互区分 -&gt; 忽略了通过隐式任务分配进行team分解的有效性   通过分层控制结构将任务分解为一组技能 或者 子任务</li>
<li><strong>methods</strong>: <ol>
<li>形式化学习目标为 role 和 它的representation 之间的互信息 -&gt; 最大限度减少role的不确定性 最小化保留role无关信息 为了简单的近似 negative pairs 的分布 -&gt; 通过编码它的trajectory到隐空间中 提取agent的行为 并根据隐空间定期地将agent分为几个簇 -&gt; 来自不同簇的点被分配为负对</li>
<li>使用注意力机制 促进 在价值分解时 global state 注意 学习到的role representation -&gt; 隐式的指导 agent 在一个 skillful role space 中协调 -&gt; 随着role的涌现生成更有表现力的 信用分配</li>
</ol>
</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li><strong>idea</strong>: 学习一个紧凑的角色表示(compact role representation) 这个compact role representation可以个性化智能体复杂的行为模式 -&gt; 使用这个角色信息可以促进个体 policy 学习 和 引导agent 协作 -&gt; 相似角色的 agents 可以通过更积极的知识传输 享受更高的学习效率 并且 不同角色的区分也可以保证智能体的异质性<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Our idea is to learn a compact role representation that can characterize complex behavior patterns of agents, and use the role information to facilitate individual policy learning and guide agent coordination. Agents with similar roles can enjoy higher learning efficiency via more aggressive knowledge transfer, and agent heterogeneity is also guaranteed with the discrimination of diverse roles.</span><br></pre></td></tr></table></figure></li>
<li><strong>definition 1:</strong> 在多智能体任务中，每个agent都和一个描述其行为模式的role Mi 相关联 -&gt; 其中Mi 通过 role representation zi 量化表示 -&gt; zi 通过 L个observation-action pairs训练 这些O-A pairs来自agent i的trajectory -&gt; πzi : O × A × Z → [0, 1] is the individual policy for agent i</li>
<li><strong>ACORM</strong>: （1）通过对比学习 学习 agents 对应的 role representations zi （2）使用注意力机制 促进 global state 关注 学习到的role patterns -&gt; 引导在high-level role space 中 技能上的智能体协调 -&gt; 促进有表现力的信用分配</li>
</ul>
<h3 id="Contrastive-Role-Representations"><a href="#Contrastive-Role-Representations" class="headerlink" title="Contrastive Role Representations"></a>Contrastive Role Representations</h3><ul>
<li><strong>Objective</strong>: 1. agents with similar behavior patterns exhibit closer role representations 2. with notably different strategies are pushed away from each other.</li>
<li><strong>key issues</strong> 1. how to define a feasible metric to quantify the degree of similaroty between agent’s behaviors 2. how to develop an efficient method to optimize the discrimination of role representations</li>
<li><strong>1 Agent Embedding</strong>:  $e^{t}_{i}=f_{\phi}(o_{i}^{t-1}, a_{i}^{t-1}, e_{i}^{t-1})$ distance between the obtained agent embeddings -&gt; metric to measure the behavior dissimilarity between agents</li>
<li><p><strong>2 Contrastive Learning</strong>: discriminative role representation &lt;- agent’s behaviors patterns   -&gt;   maximize the mutual information between the role and its representation learn a role encoder that maximally reduces role uncertainty while minimally preserving role-irrelevant information<br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886563461.png" alt="1709886563461"></p>
</li>
<li><p>$L_{CL}$:<br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886677274.png" alt="1709886677274"><br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886690507.png" alt="1709886690507"><br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886699683.png" alt="1709886699683"><br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709886709036.png" alt="1709886709036"></p>
<ul>
<li>$|M| = K$ K：we partition all n agents into K clusters $\{C_{j}\}_{j=1}^{K}$ according to agent embeddings.</li>
<li>$z^{T}_{i}Wz_{i’}$ where W is a learnable parameter matrix</li>
</ul>
</li>
<li><strong>MOCO method</strong>: maintain a query encoder θq and a key encoder θk, and use a momentum update to facilitate the key representations’ consistency as<br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709887238174.png" alt="1709887238174"><br>where β ∈ [0, 1) is a momentum coefficient, and only parameters θq are updated by backpropagation.</li>
</ul>
<h3 id="ATTENTION-GUIDED-ROLE-COORDINATION"><a href="#ATTENTION-GUIDED-ROLE-COORDINATION" class="headerlink" title="ATTENTION-GUIDED ROLE COORDINATION"></a>ATTENTION-GUIDED ROLE COORDINATION</h3><ul>
<li>global state 和 agent’s role representations 进行多头注意力机制计算: 促进global state关注学习到的 role representations -&gt; 从而在价值分解中提供更具有表现力的信用分配</li>
</ul>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709893541789.png" alt="1709893541789"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ol>
<li>Can ACORM facilitate learning efficiency and stability in complex multi-agent domains? If so, what are the respective contributions of different modules to the performance gains? (See Sec. 3.1).</li>
<li>Can ACORM learn meaningful role representations associated with agent’s behavior patterns and achieve effective dynamic team composition? (See Sec. 3.2).</li>
<li>Can ACORM successfully attend to learned role representations to realize skillful role coordination and more expressive credit assignment? (See Sec. 3.3).</li>
</ol>
<h3 id="3-1-efficiency-and-stability"><a href="#3-1-efficiency-and-stability" class="headerlink" title="3.1 efficiency and stability"></a>3.1 efficiency and stability</h3><ul>
<li><strong>performance</strong>:<br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709895486679.png" alt="1709895486679"></li>
<li>A noteworthy point is that ACORM outperforms all baselines by the largest margin on super hard maps that demand a significantly higher degree of behavior diversity and coordination: MMM2, 3s5z_vs_3s6z, and corridor.</li>
<li>ACORM exhibits the lowest variance in learning curves, signifying not only superior learning efficiency but also enhanced training stability.</li>
<li><strong>Abations</strong>:<br><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709897074004.png" alt="1709897074004"></li>
<li>It demonstrates that both components are essential for ACORM’s capability and they are complementary to each other.</li>
<li>Specifically, ACORM_w/o_MHA (Vanilla) obtains very similar performance compared to ACORM_w/o_MHA, indicating that the effectiveness comes from the attention module other than encoding the state trajectory via a GRU.</li>
</ul>
<h3 id="3-2-CONTRASTIVE-ROLE-REPRESENTATIONS"><a href="#3-2-CONTRASTIVE-ROLE-REPRESENTATIONS" class="headerlink" title="3.2 CONTRASTIVE ROLE REPRESENTATIONS"></a>3.2 CONTRASTIVE ROLE REPRESENTATIONS</h3><p><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709908700732.png" alt="1709908700732"></p>
<ul>
<li><strong>why use the contrastive learning</strong>: <ul>
<li><strong>ei v.s. zi</strong>: Initially (t = 1, 12), all agent embeddings tend to be crowded together with limited discrimination, and the K-means algorithm moderately separates them into several clusters. Via contrastive learning, the acquired role representations within the same cluster are pushed closer to each other, and those in different clusters are notably separated.</li>
<li>At a later stage (t = 40), agent embeddings are already scattered widely throughout the space with a good clustering effect so far. This phenomenon indicates that the system has learned effective role assignment with heterogeneous behavior patterns. Then, the role encoder transforms these agent embeddings into more discriminative role representations.</li>
<li>至于本文为什么要在K-means聚类的基础上再做对比学习，我认为应该是et的特征包含了很多任务无关的信息，而进行对比学习可以提取出更为抽象有用的特征zt 仅包含agent技能合作相关的信息<h3 id="3-3-ATTENTION-GUIDED-ROLE-COORDINATION"><a href="#3-3-ATTENTION-GUIDED-ROLE-COORDINATION" class="headerlink" title="3.3 ATTENTION-GUIDED ROLE COORDINATION"></a>3.3 ATTENTION-GUIDED ROLE COORDINATION</h3><img src="/2024/03/04/ATTENTION-GUIDED-CONTRASTIVE-ROLE-REPRE-SENTATIONS-FOR-MULTI-AGENT-REINFORCEMENT-LEARNING/1709909066167.png" alt="1709909066167"></li>
</ul>
</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer"><script src="//sdk.jinrishici.com/v2/browser/jinrishici.js"></script>
<script>
  jinrishici.load((result) => {
    let jrsc = document.getElementById('jrsc');
    const data = result.data;
    let author = data.origin.author;
    let title = '《' + data.origin.title + '》';
    let content = data.content.substr(0, data.content.length - 1);
    let dynasty = data.origin.dynasty.substr(0, data.origin.dynasty.length - 1);
    jrsc.innerText = content + ' @ ' + dynasty + '·' + author + title;
  });
</script>
<div style="text-align: center"><span id="jrsc" >正在加载今日诗词....</span></div>

          <div class="post-tags">
              <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag"># 论文阅读</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/02/pytorch%E4%B8%ADreshape%E5%92%8Ctranspose/" rel="prev" title="pytorch中reshape和transpose">
                  <i class="fa fa-angle-left"></i> pytorch中reshape和transpose
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/03/10/%E5%85%B3%E4%BA%8Ehexo-new/" rel="next" title="关于hexo new">
                  关于hexo new <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Wei</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">44k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:13</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
